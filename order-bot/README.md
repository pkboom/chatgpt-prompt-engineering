# Run script

```sh
streamlit run app.py
```

1 token ~= 4 chars in English

1 token ~= ¾ words

4/3 token ~= 1 words

40 tokens ~= 30 words

4097 tokens ~= 3072 words

This model’s maximum context length is 4097 tokens

# Temperature

0: the response will be very straightforward, almost deterministic (meaning you almost always get the same response to a given prompt)

1: the response can vary wildly.

# Prompt idea

prompt size can be adjusted based on categories, maybe.
but it's a bit odd.
